{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58ba0ddb",
   "metadata": {},
   "source": [
    "### Custom_GPT2_TextGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a946bcc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -Import the required libraries-\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "import torch\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import PyPDF2\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import torch\n",
    "import string\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import tensorflow as tf\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, TextDataset, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, TextDataset, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "19512857",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -Tokenizer for GPT-2-\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3c0f2b22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/m-hadi/anaconda3/envs/mycondaenv/lib/python3.11/site-packages/PyPDF2/_cmap.py:134: PdfReadWarning: Advanced encoding /Identity#2dH not implemented yet\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Book 1 - Title: Language Models are Few-Shot Learners, Author: Tom B. Brown\n",
      "\n",
      "\n",
      "Book 2 - Title: Explorations in Artificial Intelligence and Machine Learning, Author: Prof. Roberto V. Zicari\n",
      "\n",
      "\n",
      "Book 3 - Title: Artificial Intelligence A Modern Approach Third Edition, Author: Stuart J. Russell and Peter Norvig\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# -Fetch text from pdf by link-\n",
    "def fetch_text_from_pdf(pdf_link):\n",
    "    try:\n",
    "        # -Download the PDF file from the provided link-\n",
    "        response = requests.get(pdf_link)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        # -Check if the response content type is PDF-\n",
    "        if response.headers.get('content-type') == 'application/pdf':\n",
    "            # -You have successfully fetched the PDF content-\n",
    "            pdf_content = response.content\n",
    "\n",
    "            # -Create a BytesIO stream from the PDF content-\n",
    "            pdf_stream = BytesIO(pdf_content)\n",
    "\n",
    "            # -Create a PDF reader object-\n",
    "            pdf_reader = PyPDF2.PdfFileReader(pdf_stream)\n",
    "\n",
    "            # -Initialize a variable to store the extracted text-\n",
    "            extracted_text = \"\"\n",
    "\n",
    "            # -Extract text from each page of the PDF-\n",
    "            for page_num in range(pdf_reader.numPages):\n",
    "                page = pdf_reader.getPage(page_num)\n",
    "                extracted_text += page.extractText()\n",
    "            # -Return the extracted text-\n",
    "            return extracted_text \n",
    "\n",
    "        else:\n",
    "            print(\"The fetched content is not a PDF.\")\n",
    "            return None\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching text from PDF: {e}\")\n",
    "        return None\n",
    "\n",
    "def preprocess_text(text):\n",
    "    if text is None:\n",
    "        # -Return an empty string if text is None-\n",
    "        return \"\"  \n",
    "\n",
    "    # -Remove non-printable characters and Unicode escape sequences-\n",
    "    text = text.encode('ascii', 'ignore').decode('utf-8')\n",
    "    # -TODO: Add preprocess steps as per data, Convert the text to lowercase-\n",
    "    text = text.lower()\n",
    "    # -Tokenize the text into individual words-\n",
    "    tokens = word_tokenize(text)\n",
    "    # -Remove stopwords and punctuation from the tokens-\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    # -Access the punctuation characters-\n",
    "    punctuation = set(string.punctuation)  \n",
    "    tokens = [token for token in tokens if token not in stop_words and token not in punctuation]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# -Collect and preprocess data from the PDFs-\n",
    "\n",
    "# -Use a list to store preprocessed text for each book-\n",
    "corpus = []  \n",
    "\n",
    "# -List of books on Chanakya Neeti with their PDF links-\n",
    "books = [\n",
    "    {\"title\": \"Language Models are Few-Shot Learners\", \"author\": \"Tom B. Brown\", \n",
    "     \"pdf_link\": \"https://arxiv.org/pdf/2005.14165.pdf\"},\n",
    "    {\"title\": \"Explorations in Artificial Intelligence and Machine Learning\", \n",
    "     \"author\": \"Prof. Roberto V. Zicari\", \"pdf_link\": \"https://www.routledge.com/rsc/downloads/AI_FreeBook.pdf\"},\n",
    "    {\"title\": \"Artificial Intelligence A Modern Approach Third Edition\",\n",
    "     \"author\": \"Stuart J. Russell and Peter Norvig\",\n",
    "     \"pdf_link\": \"https://people.engr.tamu.edu/guni/csce421/files/AI_Russell_Norvig.pdf\"}\n",
    "    # -Add more books to the list-\n",
    "]\n",
    "\n",
    "for book in books:\n",
    "    pdf_link = book[\"pdf_link\"]\n",
    "    text = fetch_text_from_pdf(pdf_link)\n",
    "    \n",
    "    if text is not None:\n",
    "        processed_text = preprocess_text(text)\n",
    "        # -Append the preprocessed text for each book to the corpus list-\n",
    "        corpus.append(processed_text)  \n",
    "\n",
    "# -Print the preprocessed data-\n",
    "for i, book in enumerate(books):\n",
    "    print(f\"Book {i + 1} - Title: {book['title']}, Author: {book['author']}\")\n",
    "    # -You can print the preprocessed text for each book-\n",
    "    # print(corpus[i]) \n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "64cc6308",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (41381 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "# -Tokenize the preprocessed text using the GPT-2 tokenizer-\n",
    "tokenized_corpus = [tokenizer(text, return_tensors=\"pt\") for text in corpus]\n",
    "\n",
    "# -Create a custom dataset for training-\n",
    "class CustomTextDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, texts, tokenizer, block_size):\n",
    "        self.texts = texts\n",
    "        self.tokenizer = tokenizer\n",
    "        self.block_size = block_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        inputs = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.block_size,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        return {\n",
    "            \"input_ids\": inputs[\"input_ids\"].squeeze(),  # -Remove the extra dimension-\n",
    "            \"attention_mask\": inputs[\"attention_mask\"].squeeze(),\n",
    "        }\n",
    "\n",
    "block_size = 128  # -This can be changed as per requirement-\n",
    "custom_dataset = CustomTextDataset(corpus, tokenizer, block_size)\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c2f9401f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='20' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [20/20 03:42, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('./custom_gpt2_textgeneration/tokenizer_config.json',\n",
       " './custom_gpt2_textgeneration/special_tokens_map.json',\n",
       " './custom_gpt2_textgeneration/vocab.json',\n",
       " './custom_gpt2_textgeneration/merges.txt',\n",
       " './custom_gpt2_textgeneration/added_tokens.json')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the GPT-2 model\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./custom_gpt2_textgeneration\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=20,  # Adjust the number of training epochs\n",
    "    per_device_train_batch_size=32,  # Adjust batch size based on your GPU memory\n",
    "    save_steps=10_000,   # Adjust this as needed\n",
    "    save_total_limit=2,  # Only keep the last two checkpoints\n",
    "    learning_rate=2e-4,  # Adjust the learning rate\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=100,\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=custom_dataset,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the trained model\n",
    "model.save_pretrained(\"./custom_gpt2_textgeneration\")\n",
    "tokenizer.save_pretrained(\"./custom_gpt2_textgeneration\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cc9238de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/m-hadi/anaconda3/envs/mycondaenv/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/m-hadi/anaconda3/envs/mycondaenv/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "# Provide a prompt\n",
    "prompt = \"What is Artificial Neural Network\"\n",
    "\n",
    "# Generate text\n",
    "generated_text = model.generate(\n",
    "    input_ids=tokenizer.encode(prompt, return_tensors=\"pt\"),\n",
    "    max_length=100,  # Adjust the length as needed\n",
    "    num_return_sequences=1,  # Number of text sequences to generate\n",
    "    no_repeat_ngram_size=2,  # Avoid repeating n-grams in the output\n",
    "    top_k=50,  # Limit the choice of next tokens\n",
    "    top_p=0.95,  # Control randomness in the output\n",
    "    temperature=0.7,  # Adjust the temperature for randomness\n",
    ")\n",
    "\n",
    "# Decode the generated text\n",
    "generated_text = tokenizer.decode(generated_text[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "90b50ce2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is Artificial Neural Network learning machine learning 2nd ed. machine vision 3rd eduplnick tom b. brownbeard machine code prof. roberto v. zicari 1st ednicholas j. ansicommon lisp 2 bayesian networks ariel m. jared bernie profanez alicher tom henighan rewon child aditya ramesh daniel mccain wu clemens winter christopher hesse mark chen er\n"
     ]
    }
   ],
   "source": [
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed6a15f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
